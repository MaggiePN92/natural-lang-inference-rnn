{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rvBs6eLFo8vr"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from smart_open import open\n",
        "\n",
        "\n",
        "def read_tsv_as_lists(path : str) -> List[List[str]]:\n",
        "    # assumes first line is header \n",
        "    d = [line.strip().split('\\t') for line in open(path)][1:]\n",
        "\n",
        "    targets = []\n",
        "    premise = []\n",
        "    hypothesis = []\n",
        "    premise_pos = []\n",
        "    hypothesis_pos = []\n",
        "\n",
        "    for ls in d:\n",
        "\n",
        "        targets.append(ls[0])\n",
        "        premise.append(ls[1])\n",
        "        hypothesis.append(ls[2])\n",
        "        premise_pos.append(ls[3])\n",
        "        hypothesis_pos.append(ls[4])\n",
        "    # returns five lists with labels and input\n",
        "    return targets, premise, hypothesis, premise_pos, hypothesis_pos\n",
        "\n",
        "targets, premise, hypothesis, premise_pos, hypothesis_pos = parse_data(\"/content/drive/MyDrive/mnli_train.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Sjijun2pjjf"
      },
      "outputs": [],
      "source": [
        "from torch import nn \n",
        "\n",
        "\n",
        "class ANN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequential,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.sequential = sequential\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x = self.sequential(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "57EHfN9c_h4o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Embedding:\n",
        "  def __init__(\n",
        "      self,\n",
        "      embedding_weights,\n",
        "      train_embedding = False\n",
        "  ) -> None:\n",
        "      self.embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
        "      self.embedding.requires_grad = train_embedding\n",
        "\n",
        "  def mean_concat(self, premise, hypothesis, mean_dim = 0, concat_dim=1):\n",
        "    premise_emb = self.embedding(premise).mean(dim=mean_dim)\n",
        "    hypothesis_emb = self.embedding(hypothesis).mean(dim=mean_dim)\n",
        "    inputs = torch.concat((premise_emb, hypothesis_emb), dim=concat_dim)\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YjLAB9DctAtk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
        "# import requests, zipfile, io\n",
        "\n",
        "# r = requests.get(\"http://vectors.nlpl.eu/repository/20/40.zip\")\n",
        "# z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "# z.extractall(\"/content/drive/MyDrive/embedding_word2vec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gi6ngYBX0o3i"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "emb_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "    \"/content/drive/MyDrive/embedding_word2vec/model.bin\",\n",
        "    binary=True, \n",
        "    unicode_errors=\"replace\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "62q7IWq007ub"
      },
      "outputs": [],
      "source": [
        "# from os import path\n",
        "# \n",
        "# model_path = \"/content/drive/MyDrive/embedding_word2vec/\"\n",
        "# metadata_file = path.join(model_path, \"meta.json\")\n",
        "# \n",
        "# with open(metadata_file, \"r\") as meta:\n",
        "#         print(meta.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s6pqB6n3wYj1"
      },
      "outputs": [],
      "source": [
        "def get_emb_idx(w, emb_model = emb_model):\n",
        "    return emb_model.vocab[w].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nTD8YhWbvLTA"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from typing import List\n",
        "PUNCTS = string.punctuation.replace(\"'\", \"\")\n",
        "\n",
        "\n",
        "def data_prep(txt : str) -> List[str]:\n",
        "  txt = txt.lower().translate(str.maketrans(PUNCTS, ' '*len(PUNCTS)))\n",
        "  # remove ' from string with punctuations marks, other punctuation marks will\n",
        "  # be removed from string. Remaining punctuations are !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "  \n",
        "  # text is casefolded, chars in PUNCTS are removed and double white spaces are reduced\n",
        "  # to single white space\n",
        "  #txt = [t.lower().translate(str.maketrans('', '', PUNCTS)).strip() for t in txt.split()]\n",
        "  txt = [t.strip() for t in txt.split()]\n",
        "  return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HygslYOH5i6Q"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# \n",
        "# torch_emb = nn.Embedding.from_pretrained(torch.FloatTensor(emb_model.vectors))\n",
        "# test_str = data_prep(premise[0])\n",
        "# emb_test = torch_emb(torch.LongTensor([get_emb_idx(w) for w in test_str if w in emb_model.vocab]))\n",
        "# emb_test.mean(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0bQ9qi8T6rGy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "target_stoi = {'contradiction' : 1, 'entailment' : 2, 'neutral' : 0}\n",
        "\n",
        "\n",
        "class NLIDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self, \n",
        "        premise, \n",
        "        hypothesis, \n",
        "        targets,\n",
        "        target_stoi\n",
        "    ) -> None:\n",
        "        self.premise = premise\n",
        "        self.hypothesis = hypothesis\n",
        "        self.target_stoi = target_stoi\n",
        "        self.targets = [self.target_stoi[t] for t in targets]\n",
        "\n",
        "    def __getitem__(self, idx : int):\n",
        "        premise = torch.LongTensor(\n",
        "            [get_emb_idx(w) for w in self.premise[idx] if w in emb_model.vocab]\n",
        "        )\n",
        "        \n",
        "        hypothesis = torch.LongTensor(\n",
        "            [get_emb_idx(w) for w in self.hypothesis[idx] if w in emb_model.vocab]\n",
        "        )\n",
        "\n",
        "        targets = self.targets[idx]\n",
        "        return premise, hypothesis, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QUbz50_kXT6w"
      },
      "outputs": [],
      "source": [
        "class NLIDatasetConcat(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self, \n",
        "        premise, \n",
        "        hypothesis, \n",
        "        targets,\n",
        "        target_stoi,\n",
        "        embedding\n",
        "    ) -> None:\n",
        "        self.premise = premise\n",
        "        self.hypothesis = hypothesis\n",
        "        self.target_stoi = target_stoi\n",
        "        self.targets = [self.target_stoi[t] for t in targets]\n",
        "        self.embedding = embedding\n",
        "\n",
        "    def __getitem__(self, idx : int):\n",
        "        premise = torch.LongTensor(\n",
        "            [get_emb_idx(w) for w in data_prep(self.premise[idx]) if w in emb_model.vocab]\n",
        "        )\n",
        "        \n",
        "        hypothesis = torch.LongTensor(\n",
        "            [get_emb_idx(w) for w in data_prep(self.hypothesis[idx]) if w in emb_model.vocab]\n",
        "        )\n",
        "        inputs = self.embedding.mean_concat(premise, hypothesis, concat_dim=0)\n",
        "        targets = self.targets[idx]\n",
        "        return inputs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wBLDoj2a7QRC"
      },
      "outputs": [],
      "source": [
        "test_dataset = NLIDataset(\n",
        "    premise=premise[:100_000],\n",
        "    hypothesis=hypothesis[:100_000],\n",
        "    targets=targets[:100_000],\n",
        "    target_stoi=target_stoi\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HaZQS24WXzkH"
      },
      "outputs": [],
      "source": [
        "emb = Embedding(torch.FloatTensor(emb_model.vectors))\n",
        "\n",
        "test_dataset_concat = NLIDatasetConcat(\n",
        "    premise=premise,\n",
        "    hypothesis=hypothesis,\n",
        "    targets=targets,\n",
        "    target_stoi=target_stoi,\n",
        "    embedding=emb\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WGiDe8vX814",
        "outputId": "459631dc-ca71-4581-88f1-afc8db02a325"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([200])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset_concat[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgQqaMD3GJbT"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def collate_mean_emb(data):\n",
        "  premise, hypothesis, label = zip(*data)\n",
        "  label = torch.LongTensor(label)\n",
        "  premise = pad_sequence(premise)\n",
        "  hypothesis = pad_sequence(hypothesis)\n",
        "  return premise, hypothesis, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa6B5It0D0lI"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_mean_emb\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "roH8I_ybYWVk"
      },
      "outputs": [],
      "source": [
        "train_dataloader_concat = torch.utils.data.DataLoader(\n",
        "    test_dataset_concat,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lbX7P3UEJYA",
        "outputId": "c8debcf7-eb68-4beb-cbbb-d7f0a7374145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1086, 128])\n",
            "torch.Size([118, 128])\n",
            "torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "for _premise, _hypothesis, _label in train_dataloader:\n",
        "  print(_premise.shape)\n",
        "  print(_hypothesis.shape)\n",
        "  print(_label.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmbASa43YbMD",
        "outputId": "25384f70-c737-41da-bfc2-508d6bda0985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 200])\n",
            "torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "for inputs, _label in train_dataloader_concat:\n",
        "  print(inputs.shape)\n",
        "  print(_label.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSaWwaly-54U"
      },
      "outputs": [],
      "source": [
        "model_seq = nn.Sequential(\n",
        "  nn.Linear(200, 512),\n",
        "  nn.Tanh(),\n",
        "  nn.Linear(512,3),\n",
        ")\n",
        "\n",
        "model = ANN(model_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0on-nZ7Cx9R"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.1,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-08,\n",
        "    weight_decay=0.05\n",
        ")\n",
        "# use ExponetialLR as learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
        "    optimizer, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUnHWtAJC98u"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "n_epochs = 5\n",
        "loss_fn = F.cross_entropy\n",
        "accum_loss = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # put model in train mode, if drop out is included in forward this will be activated\n",
        "    model.train()\n",
        "    # get data and targets from the dataloader, these are put to the correct device\n",
        "    for _premise, _hypothesis, _labels in train_dataloader:\n",
        "        epoch_train_acc = []\n",
        "        # classes = classes.to(device)\n",
        "        # zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # make prediction \n",
        "        inputs = emb.mean_concat(\n",
        "            _premise, _hypothesis, mean_dim=0, concat_dim=1\n",
        "        )\n",
        "        y_pred = model(inputs)\n",
        "        # calcualte loss\n",
        "        loss = loss_fn(\n",
        "            y_pred, \n",
        "            _labels\n",
        "        ).mean()\n",
        "        accum_loss.append(loss.item())\n",
        "        # calculate grads \n",
        "        loss.backward()\n",
        "        # update weights w.r.t. grads \n",
        "        optimizer.step()\n",
        "        epoch_train_acc.append((y_pred.argmax(dim=1) == _labels).float().mean().item())\n",
        "\n",
        "    print(epoch,\": \" ,accum_loss[-1], sep=\"\")\n",
        "    print(round(sum(epoch_train_acc)/len(epoch_train_acc), 3))\n",
        "\n",
        "    # adjust learning rate\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "f8zoz7d5PQMz"
      },
      "outputs": [],
      "source": [
        "model_seq = nn.Sequential(\n",
        "  nn.Linear(200, 512),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(512,3),\n",
        ")\n",
        "\n",
        "model = ANN(model_seq)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-08,\n",
        "    weight_decay=0.05\n",
        ")\n",
        "# use ExponetialLR as learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
        "    optimizer, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi90eVEpYzNS",
        "outputId": "d093e387-90c2-4e68-eba1-eb83ebc6b3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: 0.8983548283576965\n",
            "0.565\n",
            "1: 0.9121233224868774\n",
            "0.652\n",
            "2: 0.9223904609680176\n",
            "0.609\n",
            "3: 0.9215961694717407\n",
            "0.652\n",
            "4: 0.9078409075737\n",
            "0.609\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "n_epochs = 5\n",
        "loss_fn = F.cross_entropy\n",
        "accum_loss = []\n",
        "expl = False \n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # put model in train mode, if drop out is included in forward this will be activated\n",
        "    model.train()\n",
        "    # get data and targets from the dataloader, these are put to the correct device\n",
        "    i = 0\n",
        "    for inputs, _labels in train_dataloader_concat:\n",
        "        \n",
        "        epoch_train_acc = []\n",
        "        # classes = classes.to(device)\n",
        "        # zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(torch.nan_to_num(inputs))\n",
        "        \n",
        "        #if not y_pred.isfinite().all().item():\n",
        "        #  expl = True\n",
        "        #  break\n",
        "        \n",
        "        i = i + 1\n",
        "        # calcualte loss\n",
        "        loss = loss_fn(\n",
        "            y_pred, \n",
        "            _labels\n",
        "        ).mean()\n",
        "        accum_loss.append(loss.item())\n",
        "        # calculate grads \n",
        "        loss.backward()\n",
        "        # update weights w.r.t. grads \n",
        "        loss.register_hook(lambda grad: print(grad))\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25, error_if_nonfinite=True)\n",
        "        optimizer.step()\n",
        "        epoch_train_acc.append((y_pred.argmax(dim=1) == _labels).float().mean().item())\n",
        "  \n",
        "\n",
        "    # if expl:\n",
        "    #   print()\n",
        "    #   break\n",
        "    print(epoch,\": \" ,accum_loss[-1], sep=\"\")\n",
        "    print(round(sum(epoch_train_acc)/len(epoch_train_acc), 3))\n",
        "    \n",
        "\n",
        "    # adjust learning rate\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lngC6nOSigHo"
      },
      "outputs": [],
      "source": [
        "for p in model.parameters():\n",
        "  print(p.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHlD0FZu4Ws9"
      },
      "outputs": [],
      "source": [
        "ids = torch.LongTensor([get_emb_idx(w) for w in data_prep(hypothesis[30462]) if w in emb_model.vocab])\n",
        "emb.embedding(ids).mean(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERW3aoSo4hHa"
      },
      "outputs": [],
      "source": [
        "ids = torch.LongTensor([get_emb_idx(w) for w in data_prep(premise[30462]) if w in emb_model.vocab])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5-Q6Ok64uug"
      },
      "outputs": [],
      "source": [
        "emb.embedding(ids).mean(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ciF1TSZj5ljK",
        "outputId": "cbd0337b-3d97-4e6f-abc9-28bf60336d50"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The beverage is full of citrus extracts and pieces of citrus.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hypothesis[7779]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qjSR_0qF8_Nu",
        "outputId": "0401ffaa-b7d0-4170-c4ff-948f324726ab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Becauseitsafullyloadedcitrusbeverageright?'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "premise[7779]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqu_LjMe5haF",
        "outputId": "46921a44-1971-44f1-cb92-2776b58dd24e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['becauseitsafullyloadedcitrusbeverageright']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_prep(premise[7779])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tH2CNwq3Yy_"
      },
      "outputs": [],
      "source": [
        "torch.nan_to_num(test_dataset_concat[7779][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IinkYeTk4KS_",
        "outputId": "0292b0c2-6650-4a01-ca0a-a734b054668e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7779\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(test_dataset_concat)):\n",
        "  if not test_dataset_concat[i][0].isfinite().all().item():\n",
        "    print(i)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIgEGLkl2AxE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "in4080_2022",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "cdd0f0c9f6adf76f8a5dbbf6be5191f36dc24709c4783acdb6d648c01c7f5854"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
