{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCeVQYkEQggL"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCBs_5PNQke-"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from smart_open import open\n",
        "\n",
        "\n",
        "def read_tsv_lists(path : str) -> List[List[str]]:\n",
        "    # assumes first line is header \n",
        "    d = [line.strip().split('\\t') for line in open(path)][1:]\n",
        "\n",
        "    targets = []\n",
        "    premise = []\n",
        "    hypothesis = []\n",
        "    premise_pos = []\n",
        "    hypothesis_pos = []\n",
        "\n",
        "    for ls in d:\n",
        "\n",
        "        targets.append(ls[0])\n",
        "        premise.append(ls[1])\n",
        "        hypothesis.append(ls[2])\n",
        "        premise_pos.append(ls[3])\n",
        "        hypothesis_pos.append(ls[4])\n",
        "    # returns five lists with labels and input\n",
        "    return targets, premise, hypothesis, premise_pos, hypothesis_pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "azcmn9bIQtT0"
      },
      "outputs": [],
      "source": [
        "targets, _, _, premise_pos, hypothesis_pos = read_tsv_lists(path=\"/content/drive/MyDrive/mnli_train.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERyE22JLUXmT",
        "outputId": "c165c75d-c685-4501-955d-6403b508f267"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "PUNCTS = string.punctuation.replace(\"'\", \"\")\n",
        "PUNCTS = string.punctuation.replace(\"_\", \"\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def dataprep(txt):\n",
        "  processed_sent = []\n",
        "\n",
        "  txt = txt.translate(str.maketrans(PUNCTS, ' '*len(PUNCTS)))\n",
        "\n",
        "  for token in txt.split():\n",
        "    \n",
        "    if token.endswith(\"_PUNCT\"):\n",
        "      continue\n",
        "    \n",
        "    pos_start = token.find(\"_\")\n",
        "    \n",
        "    if token.endswith(\"_NOUN\"):\n",
        "      processed_sent.append(\n",
        "        f\"{lemmatizer.lemmatize(token[:pos_start], wordnet.NOUN).lower()}{token[pos_start:]}\"\n",
        "      )\n",
        "    elif token.endswith(\"_VERB\"):\n",
        "      processed_sent.append(\n",
        "        f\"{lemmatizer.lemmatize(token[:pos_start], wordnet.VERB).lower()}{token[pos_start:]}\"\n",
        "      )\n",
        "    elif token.endswith(\"_ADJ\"):\n",
        "      processed_sent.append(\n",
        "        f\"{lemmatizer.lemmatize(token[:pos_start], wordnet.ADJ).lower()}{token[pos_start:]}\"\n",
        "      )\n",
        "    elif token.endswith(\"_ADV\"):\n",
        "      processed_sent.append(\n",
        "        f\"{lemmatizer.lemmatize(token[:pos_start], wordnet.ADV).lower()}{token[pos_start:]}\"\n",
        "      )\n",
        "    else:\n",
        "      processed_sent.append(\n",
        "          f\"{token[:pos_start].lower()}{token[pos_start:]}\")\n",
        "    \n",
        "  return processed_sent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EgcRkQC3aS55"
      },
      "outputs": [],
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "r = requests.get(\"http://vectors.nlpl.eu/repository/20/10.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDX0elLc6jPn"
      },
      "outputs": [],
      "source": [
        "!pip install fastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPtRywxZ384M"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "#import gensim\n",
        "\n",
        "#gensim.models.fasttext.load_facebook_model('model.bin')\n",
        "model = FastText.load_binary_data('model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap4myUIy5xPR"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "model = FastText.load_facebook_vectors('model.txt', encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdo8VgkX86_-"
      },
      "outputs": [],
      "source": [
        "from gensim.models import fasttext\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "#wv = fasttext.load_facebook_vectors(datapath(\"model.bin\"))\n",
        "#wv = fasttext.load_facebook_model(\"model.bin\")\n",
        "fasttext.load(\"model.bin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA62qq-19bOy"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText, fasttext\n",
        "\n",
        "model = FastText.load(\"model.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMBjk2W79bKP"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "fasttext.load_model('model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "30KTsLd29bHZ"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gvp7PI7EWgGq"
      },
      "outputs": [],
      "source": [
        "latest = requests.get(\"http://vectors.nlpl.eu/repository/latest.json\")\n",
        "latest_json = latest.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ic3QL_k4T7op"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['algorithms', 'corpora', 'corpora_index', 'models', 'models_index']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[k for k in latest_json.keys()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n",
            "13\n",
            "15\n",
            "17\n",
            "19\n",
            "21\n"
          ]
        }
      ],
      "source": [
        "for a in latest_json[\"models\"]:\n",
        "    if 3 in a[\"corpus\"]:\n",
        "        print(a[\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "3\n",
            "75\n",
            "131\n"
          ]
        }
      ],
      "source": [
        "for a in latest_json[\"corpora\"]:\n",
        "    if a[\"NER\"]: continue\n",
        "    if not a[\"language\"] == \"eng\": continue\n",
        "    if not a[\"lemmatized\"]: continue\n",
        "    if a[\"tagset\"]: continue\n",
        "\n",
        "    print(a[\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NER': False, 'case preserved': True, 'description': 'Gigaword 5th Edition', 'id': 3, 'language': 'eng', 'lemmatized': True, 'public': False, 'stop words removal': 'NLTK', 'tagger': 'Stanford Core NLP v. 3.6.0', 'tagset': None, 'tokens': 4815382730, 'tool': None, 'url': 'https://catalog.ldc.upenn.edu/LDC2011T07'}\n"
          ]
        }
      ],
      "source": [
        "for c in latest_json[\"corpora\"]:\n",
        "    if c[\"id\"] == 3:\n",
        "        print(c)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "in4080_2022",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "cdd0f0c9f6adf76f8a5dbbf6be5191f36dc24709c4783acdb6d648c01c7f5854"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
